{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPohe2XrKxct1WaJ8bJeS+D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jainlo/faseeh/blob/main/nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOuG0edDlzRj",
        "outputId": "0f6332e2-126b-4dcd-9b28-5c668d6ff50b"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/') \n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "#to break text down into word\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "#to break text down into sentences\n",
        "from nltk.tokenize import sent_tokenize\n",
        "#to remove punctuations\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "#frequency and dictionary\n",
        "from nltk.probability import FreqDist\n",
        "from nltk import ngrams\n",
        "#should we remove stop words?\n",
        "nltk.download('stopwords')\n",
        "#from nltk.corpus import stopwords\n",
        "#convert words into their original form, is this useful for us?\n",
        "#from nltk.stem import PorterStemmer\n",
        "#split tokens by space\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "#use to find bigrams (pairs of words)\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "#measure co-occurances\n",
        "from nltk.metrics import BigramAssocMeasures"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOZmFeCFtc_R"
      },
      "source": [
        "#retrieve all files in a folder(directory) and return in array\n",
        "def listAllFiles(dir, ext):    \n",
        "    allfiles = []\n",
        "    for root, subdirs, files in os.walk(dir):\n",
        "   \n",
        "        for filename in files:\n",
        "            file_path = os.path.join(root, filename)\n",
        "            if file_path.endswith(ext):\n",
        "                if '.git' not in file_path:\n",
        "                    allfiles.append(file_path)\n",
        "\n",
        "    return allfiles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBkOpMv6t4CQ"
      },
      "source": [
        "#retrieve the content of each file and return as text\n",
        "def readFileContent(filename):\n",
        "    # Open the file as f.\n",
        "    # The function readlines() reads the file.\n",
        "    with open(filename) as f:\n",
        "        #read file line by line\n",
        "        lines = f.readlines()\n",
        "        #convert array to string for readability \n",
        "        finalText = listToText(lines)\n",
        "        #finalText = os.linesep.join([s for s in finalText.splitlines() if s])\n",
        "    return finalText\n",
        "\n",
        "def listToText(lines):\n",
        "    #create string\n",
        "    listString = \"\"\n",
        "    #loop through the list\n",
        "    for i in range(len(lines)):\n",
        "        #concatenate each list item to the string \n",
        "        listString += lines[i]\n",
        "        listString += \"\\n\"\n",
        "    #return string\n",
        "    return listString"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFf7LdeEA0AW"
      },
      "source": [
        "#token manipulation\n",
        "def t(text):\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOXIkLIAtEnc"
      },
      "source": [
        "#FIRST STEP\n",
        "#SPLIT BY WORD - GIVE FREQUENCY OF EACH WORD\n",
        "def main():\n",
        "    \n",
        "    path = \"/content/drive/MyDrive/newVersion/corpus/Aljazirah/J201801-clean/www.al-jazirah.com/2018/20180101\"\n",
        "    #save all files in that directory in an array\n",
        "    allFiles = listAllFiles(path, \".txt\")\n",
        "    for file in allFiles:\n",
        "        content = readFileContent(file)\n",
        "        #remove punctuation before tokenize\n",
        "\n",
        "        #tokenize\n",
        "        tokenized_text = wordpunct_tokenize(content)\n",
        "        print(tokenized_text)\n",
        "        #remove stopwords after tokenize\n",
        "        \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU-GRpPn-b_M"
      },
      "source": [
        "#test here\n",
        "    # This code creates tokens of words\n",
        "path = \"/content/drive/MyDrive/newVersion/corpus/Aljazirah/J201801-clean/www.al-jazirah.com/2018/20180101\"\n",
        "allFiles = listAllFiles(path, \".txt\")\n",
        "content = readFileContent(allFiles[0])\n",
        "#print(content)\n",
        "tokenized_text = wordpunct_tokenize(content)\n",
        "print(tokenized_text)\n",
        "\n",
        "    # This code counts the frequency of each token\n",
        "unigram = ngrams(tokenized_text, 1)\n",
        "\n",
        "unigram_c = {}\n",
        "for i in unigram:\n",
        "    if i not in unigram_c:\n",
        "        unigram_c[i] = 1\n",
        "    else:\n",
        "        unigram_c[i] += 1\n",
        "\n",
        "# print(bigrams_c)\n",
        "\n",
        "\n",
        "    # This code counts the frequency of each token \n",
        "\n",
        "data_analysis = nltk.FreqDist(tokenized_text)\n",
        "filter_words = dict([(m, n) for m, n in data_analysis.items() if len(m) > 1])\n",
        " \n",
        "for key in sorted(filter_words):\n",
        "    print(\"(%s, %s)\" % (key, filter_words[key]))\n",
        " \n",
        "data_analysis = nltk.FreqDist(filter_words)\n",
        " \n",
        "data_analysis.plot(25, cumulative=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-6-xSubBcGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44993289-568f-4b2c-e366-f4e2b4eb732f"
      },
      "source": [
        "#break into words\n",
        "data = \"I pledge to be a data scientist one day\"\n",
        "tokenized_text = word_tokenize(data)\n",
        "print(tokenized_text)\n",
        "print(type(tokenized_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'pledge', 'to', 'be', 'a', 'data', 'scientist', 'one', 'day']\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmXD-sIcl_BA",
        "outputId": "8133a63f-51b7-4c42-f4d4-60b844bb1b61"
      },
      "source": [
        "#break into sentences (ends with .)\n",
        "def sen\n",
        "    para=\"\"\"Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards, and pies.The most commonly used cake ingredients include flour, sugar, eggs, butter or oil or margarine, a liquid, and leavening agents, such as baking soda or baking powder. Common additional ingredients and flavourings include dried, candied, or fresh fruit, nuts, cocoa, and extracts such as vanilla, with numerous substitutions for the primary ingredients.Cakes can also be filled with fruit preserves, nuts or dessert sauces (like pastry cream), iced with buttercream or other icings, and decorated with marzipan, piped borders, or candied fruit.\"\"\"\n",
        "    tokenized_para=sent_tokenize(para)\n",
        "    print(tokenized_para)\n",
        "    print(type(tokenized_para))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Cake is a form of sweet food made from flour, sugar, and other ingredients, that is usually baked.In their oldest forms, cakes were modifications of bread, but cakes now cover a wide range of preparations that can be simple or elaborate, and that share features with other desserts such as pastries, meringues, custards, and pies.The most commonly used cake ingredients include flour, sugar, eggs, butter or oil or margarine, a liquid, and leavening agents, such as baking soda or baking powder.', 'Common additional ingredients and flavourings include dried, candied, or fresh fruit, nuts, cocoa, and extracts such as vanilla, with numerous substitutions for the primary ingredients.Cakes can also be filled with fruit preserves, nuts or dessert sauces (like pastry cream), iced with buttercream or other icings, and decorated with marzipan, piped borders, or candied fruit.']\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zBza17Dmchb",
        "outputId": "aa90dc2c-a794-4dda-e72e-f81aaa3c1779"
      },
      "source": [
        "#remove punctuation\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "result = tokenizer.tokenize(\"Wow! I am excited to learn data science\")\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Wow', 'I', 'am', 'excited', 'to', 'learn', 'data', 'science']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hzLFWO0avM1"
      },
      "source": [
        "def get_bigrams(myString):\n",
        "    #remove punctuation\n",
        "    tokenizer = WordPunctTokenizer()\n",
        "    tokens = tokenizer.tokenize(myString)\n",
        "    #find bigrams\n",
        "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
        "    #use after bigrams\n",
        "    bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 500)\n",
        "    #\n",
        "    for bigram_tuple in bigrams:\n",
        "        x = \"%s %s\" % bigram_tuple\n",
        "        tokens.append(x)\n",
        "\n",
        "    result = [x for x in tokens if x not in nltk.corpus.stopwords.words(\"arabic\")]\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orNx9kDUh9d4",
        "outputId": "d403cbc1-c610-485d-d011-ebff935113e5"
      },
      "source": [
        "def main():\n",
        "    #para=\"الطبعة الأولي بعد العرب والهوية الجغرافية والعلمية د عبدالرحمن بن سعود الهواوي\"\n",
        "    #tokenized_para=sent_tokenize(para)\n",
        "    s = \"س\"\n",
        "\n",
        "    for line in s:\n",
        "        features = get_bigrams(line)\n",
        "        print(features)\n",
        "    # train set here\n",
        "    \n",
        "\n",
        "    #arb_stopwords = set(nltk.corpus.stopwords.words(\"arabic\"))\n",
        "    #print(arb_stopwords)\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['س']\n",
            "{'كليهما', 'ولو', 'بيد', 'اللذان', 'إي', 'به', 'إليكم', 'اللائي', 'ذواتي', 'كيفما', 'ها', 'لعل', 'أنت', 'مما', 'لهم', 'كلتا', 'عليك', 'تلكم', 'مه', 'ألا', 'أنى', 'كأنما', 'لكي', 'حين', 'لكيلا', 'حيث', 'ذات', 'كليكما', 'إما', 'ذانك', 'ليس', 'ولكن', 'إلى', 'عن', 'عند', 'الذين', 'لست', 'هيت', 'إنه', 'فيما', 'تلكما', 'غير', 'إذما', 'اللتيا', 'ذا', 'عل', 'ذاك', 'خلا', 'كم', 'كذلك', 'شتان', 'لهما', 'في', 'والذي', 'أولئك', 'سوى', 'هؤلاء', 'لدى', 'وإن', 'إيه', 'ريث', 'كيت', 'مذ', 'أنتم', 'بكما', 'ممن', 'اللواتي', 'هيا', 'أوه', 'اللاتي', 'أقل', 'لستما', 'أيها', 'لولا', 'هاته', 'لئن', 'كلما', 'كلا', 'وما', 'إليكما', 'بكن', 'بماذا', 'بكم', 'حبذا', 'هنا', 'كأي', 'ذه', 'أين', 'أن', 'أو', 'مع', 'إذ', 'هم', 'فإن', 'بنا', 'عدا', 'هاهنا', 'هيهات', 'ذوا', 'وهو', 'ذينك', 'لي', 'هذه', 'إن', 'نعم', 'وإذ', 'ذلكما', 'اللتان', 'إنما', 'بي', 'أنتما', 'هل', 'ما', 'تينك', 'لهن', 'هي', 'هذا', 'بعض', 'إنا', 'بهن', 'هاتان', 'لم', 'كل', 'هاتي', 'لستم', 'فمن', 'بخ', 'بمن', 'تي', 'هذي', 'ذو', 'لستن', 'هناك', 'ومن', 'آي', 'أي', 'لما', 'هنالك', 'كأن', 'لاسيما', 'مهما', 'من', 'أنا', 'فيها', 'إليك', 'كيف', 'هكذا', 'دون', 'متى', 'هن', 'إلا', 'أولاء', 'عليه', 'لوما', 'ولا', 'فلا', 'والذين', 'لو', 'بها', 'وإذا', 'الذي', 'ليسا', 'اللتين', 'ليت', 'أم', 'بهم', 'ليسوا', 'ليستا', 'بما', 'يا', 'لها', 'عما', 'لكن', 'ذي', 'ثمة', 'فيه', 'فإذا', 'منه', 'آه', 'هاتين', 'هلا', 'له', 'تين', 'بين', 'ذلكن', 'فيم', 'حيثما', 'كلاهما', 'كما', 'كذا', 'بك', 'إذا', 'إليكن', 'هذين', 'لسن', 'بل', 'هو', 'ثم', 'ذواتا', 'ليست', 'بهما', 'عسى', 'اللذين', 'أكثر', 'ماذا', 'منها', 'تلك', 'لك', 'على', 'نحن', 'هاك', 'سوف', 'حتى', 'قد', 'أينما', 'لن', 'هذان', 'ذلك', 'كأين', 'نحو', 'كي', 'أما', 'حاشا', 'لا', 'هما', 'ذان', 'منذ', 'لسنا', 'أف', 'لنا', 'التي', 'بس', 'لكنما', 'ذين', 'ذلكم', 'ته', 'لكما', 'إذن', 'بعد', 'آها', 'أنتن', 'بلى', 'لكم'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jgw3_zoQqUPq"
      },
      "source": [
        "1.   هل يحتاج إزالة واو العطف من الكلمة\n",
        "2.   مشكلة عدم حذف ستوب وردز العربيه\n",
        "3.   مشكلة اسم عبدالله\n",
        "\n",
        "---\n",
        "\n"
      ]
    }
  ]
}